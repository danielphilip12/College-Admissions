





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor

from sklearn.metrics import root_mean_squared_error


pd.set_option('display.max_column', None)





df = pd.read_csv('./data/cleaned_college.csv')


df.head()


df['private'] = df['private'].map({'Yes': 1, 'No': 0})
# converts 'private' to a binary column, 1 for private, 0 for public. 


plt.figure(figsize=(16, 10))
sns.heatmap(df.corr(numeric_only=True),
           vmin=-1,
           vmax=1,
           cmap='coolwarm',
           annot=True)
# create a heatmap of teh correlation coefficents in the dataset. 


df.head()





features = ['private', 'apps', 'accept', 
            'room_board', 'personal', 'phd', 
            's_f_ratio', 'outstate', 'grad_rate']
# these are the features chosen for this training. 





X = df[features] # independant variables
y = df['enroll'] # dependant variable


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
# splits the data into training and testing data, using a 75/25 split. 


kf = KFold(n_splits=5, shuffle=True, random_state=42)
# this creates a KFold cross validator to split the data into 5 folds








lr = LinearRegression()
# instantiates the linear regression model. 


lr.fit(X_train, y_train)
# fits the model with the training data


lr.score(X_train, y_train)
# r2 score of the model on the training data


lr.score(X_test, y_test)
# r2 score of the model on the testing data


lr_preds = lr.predict(X_test)
# the model's predictions of the testing set


root_mean_squared_error(y_test, lr_preds)
# the rmse score of the model 


baseline = np.full_like(y_test, y_test.mean())
# baseline predictions to compare the models to (predicts mean for every entry)


root_mean_squared_error(y_test, baseline)
# baseline rmse score. 








rf = RandomForestRegressor(random_state=42)
# instantiates the random forest model


rf.fit(X_train, y_train)
# fits the model on the testing data


rf.score(X_train, y_train)
# r2 score of the training data


rf.score(X_test, y_test)
# r2 score of the testing data


rf_preds = rf.predict(X_test)
# predictions made by the random forest model


root_mean_squared_error(y_test, rf_preds)
# rmse score of the model.








from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()
# creates a scaler to scale the inputs. 


X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# scales the training and testing data


knn = KNeighborsRegressor()
# instnaites the KNN regressor model. 


param_grid = {'n_neighbors': np.arange(3, 31, 2), 
              'weights': ['uniform', 'distance'], 
              'metric': ['minkowski', 'manhattan']}
# different parameters to test with KNN


grid_knn = GridSearchCV(knn, param_grid, cv=kf)
# this will train multiple KNN models, with different combonations of 
# parameters defined by the param_grid. 
# it will also utilize the KFold initialized at the beginning for cross validation. 


grid_knn.fit(X_train_scaled, y_train)
# fits the model(s)


grid_knn.best_score_
# gets teh best score out of all the models. 


grid_knn.best_params_
# tells us what parameters made the best model 


grid_knn.score(X_test_scaled, y_test)
# gets the r2 score of the testing data (scaled)


knn_preds = grid_knn.predict(X_test_scaled)
# creates predictions with the best knn model. 


root_mean_squared_error(y_test, knn_preds)
# rmse score of KNN











import pickle


# with open('model.pkl', 'wb') as file:
#     pickle.dump(lr, file) 
#     # puts the linear regression model in a pickle file for later use. 


sns.scatterplot(x=y_test, y=lr_preds)


min_val = min(y_test.min(), lr_preds.min())
max_val = max(y_test.max(), lr_preds.max())
# creates the 2 points on the bottom left and top right for a perfect line. 

plt.plot([min_val, max_val], [min_val, max_val], linestyle='--', color='red')
# plots the perfect line

plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Predicted vs Actual")
plt.show()






